# -*- coding: utf-8 -*-
"""B19EE058_B19CSE116_CODE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XATqymfe8-dBMGbx88c3GP8nSyZrEBuQ

#Data Preprocessing
"""

#importing libraries
from sklearn.model_selection import train_test_split
import numpy as np
import os
import PIL
import cv2
import pickle

#mount drive to access data
from google.colab import drive
drive.mount('/content/drive')

"""##Feature Extraction##

###Converting image data into array data
"""

DATADIR = '/content/drive/MyDrive/dataset'
CATEGORIES = ["with_mask", "without_mask"]
IMG_SIZE = 64 
#The size of the image

#data input
X = []
#labels(0 or 1)
y = []


#function to create data from dataset directory
def create_data():
    for category in CATEGORIES:
        path = os.path.join(DATADIR, category)
        class_num_label = CATEGORIES.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)
                img_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))
                X.append(img_array)
                y.append(class_num_label)
            except Exception as e:
                pass
            
create_data()

"""##Convert to Numpy##

###The data needs to be converted in computer readable form
"""

SIZE = len(y)
data = np.array(X).flatten().reshape(SIZE, IMG_SIZE*IMG_SIZE) # pixel-features

# Turn X and y into numpy arrays
X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE) # images
y = np.array(y) # target

print("Features, X shape: ", X.shape)
print("Target, y shape: ", y.shape)
print("Data shape: ", data.shape)

"""##Plot DataSet Image##"""

# Commented out IPython magic to ensure Python compatibility.
#import matplotlib.image as mpimg
import matplotlib.pyplot as plt # to plot inage, graph
# %matplotlib inline

#drawing input images
plt.figure(figsize=(20,10))
col =5
for i in range(5):
    plt.subplot(5 / col + 1, col, i + 1)
    plt.imshow(X[i],cmap=plt.cm.gray_r,interpolation='nearest')
    #image = mpimg.imread(X[i])
    #plt.imshow(image)
    #plt.show()

"""##Save Dataset as Pickles##

###Pickle is used to tackle the scenario of regenerating data again and again
"""

#Saves us from having to regenerate our data by saving our data
pickle_out = open("X.pickle", "wb")
pickle.dump(X, pickle_out)
pickle_out.close()

pickle_out = open("y.pickle", "wb")
pickle.dump(y, pickle_out)
pickle_out.close()

pickle_out = open("data.pickle", "wb")
pickle.dump(data, pickle_out)
pickle_out.close()

"""##Load Pickles##"""

#Loading pickles using open
pickle_in = open("X.pickle", "rb")
X = pickle.load(pickle_in)
pickle_in = open("y.pickle", "rb")
y = pickle.load(pickle_in)
pickle_in = open("data.pickle", "rb")
data = pickle.load(pickle_in)

"""##Total Classification Counts##"""

print('Number of Samples:', len(y))
print('Number of Masked samples:', (y == 0).sum())
print('Number of Unmasked samples:', (y == 1).sum())

"""##Split Train and Test##"""

# Split our data into testing and training.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)

print('Length of Training data: ',len(X_train), '\nLength of Testing data: ',len(X_test))

"""#KNN

##Imports##
"""

# Commented out IPython magic to ensure Python compatibility.
# import packages
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.decomposition import PCA # Dimensionality Reduction

import seaborn as sns # for confusion matrix
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt # to plot inage, graph

import pickle
import time # for computation time assessment
# %matplotlib inline

"""##Prepare Dataset##"""

pickle_in = open("X.pickle", "rb")
X = pickle.load(pickle_in)

pickle_in = open("y.pickle", "rb")
y = pickle.load(pickle_in)

pickle_in = open("data.pickle", "rb")
data = pickle.load(pickle_in) # Data Matrix will serve as X

"""##Dataset Classification Distribution##"""

print('Number of Samples:', len(y))
print('Number of Masked samples:', (y == 0).sum())
print('Number of UnMasked samples:', (y == 1).sum())

"""##Converting Numpy to Dataframe##"""

# Get Column Names
cols = []
for i in range(0, len(data[0])):
    cols.append("P" + str(i))

# Convert to Dataframe
numpy_data = data
X = pd.DataFrame(data=numpy_data, columns=[cols])
print(X.head())

y = pd.DataFrame(data=y, columns=["Label"])
print(y.head())

"""##Data Shape##"""

# Shape
print('\nImage Data Shape:', X.shape)
print('Image Data Shape Features:', data.shape)
print('Image Data Shape Target:', y.shape)

"""
##Normalize the Data"""

X = X / 255.0

"""##Split Train + Test, random_state"""

# Split our data into testing and training.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)

# Print the length and width of our testing data.
print('Length of our Training data: ', len(X_train), '\nLength of our Testing data: ', len(X_test))

"""##KNN Model"""

# Initialize KNN model
knn = KNeighborsClassifier()

# Use training data to fit KNN model
knn.fit(X_train, y_train.values.ravel())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # make prediction on entire test data
# predictions_set1 = knn.predict(X_test)

# Save Predictions in a pickle
pickle_out = open("predictions_set1.pickle", "wb")
pickle.dump(predictions_set1, pickle_out)
pickle_out.close()

"""##Performance Metrics """

print('KNN Accuracy: %.3f' % accuracy_score(y_test, predictions_set1))

#Calculate Confusion Matrix
cm = confusion_matrix(y_test, predictions_set1)

plt.figure(figsize=(9,9))
# Heatmap visualization of accuracy
sns.heatmap(cm,annot=True, fmt='.3f', linewidths=.5, square=True,cmap='Reds_r')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, predictions_set1))
plt.title(all_sample_title,size=15)

"""##Precision, Recall, F1 Score & Classification Report##"""

print('KNN Precision: %.3f' % precision_score(y_test, predictions_set1, average='micro'))
print('KNN Recall: %.3f' % recall_score(y_test, predictions_set1, average='micro'))
print('KNN F1 Score: %.3f' % f1_score(y_test, predictions_set1, average='micro'))
print("\nNo Hyperparameter Tuning Classification Report\n", classification_report(y_test, predictions_set1))

"""#SVM

##Import Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# For data management
import pandas as pd
import numpy as np
import pickle

from sklearn import svm
from sklearn import metrics

from sklearn.model_selection import train_test_split

# For plotting
import matplotlib.pyplot as plt
# %matplotlib inline

"""##Load dataset"""

pickle_in = open("X.pickle", "rb")
X = pickle.load(pickle_in)
pickle_in = open("y.pickle", "rb")
y = pickle.load(pickle_in)

"""##Test train split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

"""##Model Building"""

nsamples, nx, ny = X_train.shape
X_train = X_train.reshape((nsamples,nx*ny))
model_100 = svm.SVC()
model_100.fit(X_train, y_train)

"""##Model Evaluation"""

# 1. Get and print a baseline accuracy score.
nsamples1, nx1, ny1 = X_test.shape
X_test = X_test.reshape((nsamples1,nx1*ny1))
y_pred = model_100.predict(X_test)
accuracy = model_100.score(X_test, y_test)
print("Accuracy %f" % accuracy)
metrics.accuracy_score(y_true=y_test, y_pred=y_pred)

print(metrics.classification_report(y_test, y_pred))

import sklearn
svm_cm = sklearn.metrics.confusion_matrix(y_test, y_pred)

plt.figure(figsize=(9, 9))
sns.heatmap(svm_cm, annot=True, fmt='.0f', square=True, linewidths=.5, cmap='Greens_r')
plt.ylabel('Actual')
plt.xlabel('Predicted')

print(sklearn.metrics.classification_report(y_test, y_pred))

"""#CNN

##Import Packages
"""

# Import Libraries 
import numpy as np
import matplotlib.pyplot as plt

import tensorflow
import sklearn
from sklearn.model_selection import train_test_split
import cv2

import seaborn as sns
import pickle
import os

"""##Load in the dataset that was preprocessed from local directory"""

# Load in the files from Google Drive 

filename = '/content/X.pickle'
pickle_in = open(filename, 'rb')
X = pickle.load(pickle_in)

filename = '/content/y.pickle'
pickle_in = open(filename, 'rb')
y = pickle.load(pickle_in)

CATEGORIES = ['masked', 'unmasked']

resized_X = []
for img in X:
    resized_X.append(cv2.resize(img, (64, 64)))

X = np.asarray(resized_X)
X = X.reshape(-1, 64, 64, 1)
print(X.shape)

X = X / 255.0

IMG_DIM = X.shape[1]
print('IMG_DIM:',IMG_DIM)

# Split dataset into Training and Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

print('Training Size:', len(X_train))
print('Testing  Size:', len(X_test))

cnn_model = tensorflow.keras.models.Sequential()

# Start of Convolution Layers & Maxpooling
cnn_model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=(IMG_DIM, IMG_DIM, 1)))
cnn_model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
cnn_model.add(tensorflow.keras.layers.MaxPool2D())

cnn_model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
cnn_model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
cnn_model.add(tensorflow.keras.layers.MaxPool2D())

cnn_model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=(IMG_DIM, IMG_DIM, 1)))
cnn_model.add(tensorflow.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
cnn_model.add(tensorflow.keras.layers.MaxPool2D())

# Start of Neural Nets
cnn_model.add(tensorflow.keras.layers.Flatten())

cnn_model.add(tensorflow.keras.layers.Dense(512, activation='relu'))
cnn_model.add(tensorflow.keras.layers.Dropout(0.3))
cnn_model.add(tensorflow.keras.layers.Dense(512, activation='relu'))
cnn_model.add(tensorflow.keras.layers.Dropout(0.3))
cnn_model.add(tensorflow.keras.layers.Dense(256, activation='relu'))
cnn_model.add(tensorflow.keras.layers.Dense(128, activation='relu'))
cnn_model.add(tensorflow.keras.layers.Dense(3, activation='softmax'))

cnn_model.summary()

# Compile the Model
cnn_model.compile(optimizer=tensorflow.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])

# Train Model
epochs = 5
cnn_model.fit(X_train, y_train, epochs=epochs, validation_split=0.1)

# Evaluate performance
cnn_model.evaluate(X_test, y_test)

y_pred = np.argmax(cnn_model.predict(X_test), axis=-1)

cm = sklearn.metrics.confusion_matrix(y_test, y_pred)

plt.figure(figsize=(9, 9))
sns.heatmap(cm, annot=True, fmt='.0f', square=True, linewidths=.5, cmap='Blues_r')
plt.ylabel('Actual')
plt.xlabel('Predicted')

print(sklearn.metrics.classification_report(y_test, y_pred))